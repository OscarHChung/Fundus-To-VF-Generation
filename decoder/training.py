#!/usr/bin/env python3
"""
ULTIMATE Training Strategy for Sub-3 MAE
Combining multiple advanced techniques:
1. Stochastic Weight Averaging (SWA)
2. Heavy augmentation with AutoAugment-style policies
3. Multi-scale TTA (8 augmentations)
4. Gradient accumulation for larger effective batch
5. Cosine annealing with warm restarts
6. Lookahead optimizer
7. Label smoothing via regression
8. Self-distillation
"""

import os, sys, json, numpy as np
import argparse
from typing import List, Tuple
import copy
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # Non-interactive backend

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image, ImageEnhance
from tqdm import tqdm
import random

# ============== MPS Configuration ==============
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'

if torch.backends.mps.is_available():
    DEVICE = torch.device("mps")
    print(f"‚úì Using MPS (Apple Silicon GPU)")
else:
    DEVICE = torch.device("cpu")
    print(f"‚ö†Ô∏è  MPS not available, using CPU")

# ============== Config ==============
BATCH_SIZE = 24  # Smaller for gradient accumulation
ACCUM_STEPS = 2  # Effective batch size = 48
EPOCHS = 300
LR = 2.5e-3
WEIGHT_DECAY = 3e-4
PATIENCE = 70
NUM_ENCODER_BLOCKS = 6
MASKED_VALUE_THRESHOLD = 99.0
DROPOUT_RATE = 0.45
USE_TTA = True
NUM_TTA_AUGS = 8  # Much more TTA
USE_SWA = True  # Stochastic Weight Averaging
SWA_START = 100  # Start SWA after epoch 100
LABEL_SMOOTH = 0.1  # Regression label smoothing
USE_OUTLIER_CLIPPING = True  # Clip extreme predictions
OUTLIER_CLIP_RANGE = (0, 35)  # Clip predictions to valid VF range
USE_ROBUST_LOSS = True  # Use Huber loss (less sensitive to outliers)
USE_MIXUP = False 
MIXUP_ALPHA = 0.2

# Paths
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
RETFOUND_DIR = os.path.join(CURRENT_DIR, '..', 'encoder', 'RETFound_MAE')
CHECKPOINT_PATH = os.path.join(CURRENT_DIR, "..", "encoder", "RETFound_cfp_weights.pth")
PRETRAINED_DECODER = os.path.join(CURRENT_DIR, "pretrained_vf_decoder.pth")
BASE_DIR = os.path.join(CURRENT_DIR, "..")
FUNDUS_DIR = os.path.join(BASE_DIR, "data", "fundus", "grape_fundus_images")
TRAIN_JSON = os.path.join(BASE_DIR, "data", "vf_tests", "grape_train.json")
VAL_JSON = os.path.join(BASE_DIR, "data", "vf_tests", "grape_test.json")
BEST_SAVE = os.path.join(CURRENT_DIR, "best_multi_image_model.pth")
INFERENCE_SAVE = os.path.join(CURRENT_DIR, "inference_model.pth")
DEBUG_DIR = os.path.join(CURRENT_DIR, "debug_visualizations")
os.makedirs(DEBUG_DIR, exist_ok=True)

# ============== Mask ==============
mask_OD = np.array([
    [False, False, False, True,  True,  True,  True,  False, False],
    [False, False, True,  True,  True,  True,  True,  True,  False],
    [False, True,  True,  True,  True,  True,  True,  True,  True ],
    [True,  True,  True,  True,  True,  True,  True,  False, True ],
    [True,  True,  True,  True,  True,  True,  True,  False, True ],
    [False, True,  True,  True,  True,  True,  True,  True,  True ],
    [False, False, True,  True,  True,  True,  True,  True,  False],
    [False, False, False, True,  True,  True,  True,  False, False]
], dtype=bool)

valid_indices_od = [i for i, v in enumerate(mask_OD.flatten()) if v]
valid_indices_os = list(reversed(valid_indices_od))

# ============== Load RETFound ==============
sys.path.insert(0, RETFOUND_DIR)
from models_mae import mae_vit_large_patch16_dec512d8b

with torch.serialization.safe_globals([argparse.Namespace]):
    checkpoint = torch.load(CHECKPOINT_PATH, map_location='cpu', weights_only=False)

base_model = mae_vit_large_patch16_dec512d8b()
base_model.load_state_dict(checkpoint['model'], strict=False)
print(f"‚úì Loaded RETFound")

pretrained_decoder_state = None
if os.path.exists(PRETRAINED_DECODER):
    try:
        with torch.serialization.safe_globals([np.dtype]):
            pretrained_checkpoint = torch.load(PRETRAINED_DECODER, map_location='cpu', weights_only=False)
        pretrained_decoder_state = pretrained_checkpoint['model_state_dict']
        print(f"‚úì Pre-trained decoder: {pretrained_checkpoint['val_mae']:.2f} dB")
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not load pre-trained decoder: {e}")

# ============== Heavy Augmentation ==============
class RandAugment:
    """Custom strong augmentation policy"""
    def __init__(self):
        self.ops = [
            lambda img: ImageEnhance.Brightness(img).enhance(random.uniform(0.7, 1.3)),
            lambda img: ImageEnhance.Contrast(img).enhance(random.uniform(0.7, 1.3)),
            lambda img: ImageEnhance.Color(img).enhance(random.uniform(0.7, 1.3)),
            lambda img: ImageEnhance.Sharpness(img).enhance(random.uniform(0.7, 1.3)),
        ]
    
    def __call__(self, img):
        n_ops = random.randint(1, 3)
        for _ in range(n_ops):
            op = random.choice(self.ops)
            img = op(img)
        return img

train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(p=0.3),  # Reduced from 0.5
    transforms.RandomRotation(10),  # Reduced from 20
    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.95, 1.05)),  # Reduced
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    # Remove RandomErasing and RandAugment for now
])

val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# ============== 8x TTA ==============
def get_tta_transforms():
    """8 diverse augmentations for TTA"""
    return [
        val_transform,
        transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.Lambda(lambda img: transforms.functional.hflip(img)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ]),
        transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.Lambda(lambda img: transforms.functional.rotate(img, 5)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ]),
        transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.Lambda(lambda img: transforms.functional.rotate(img, -5)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ]),
        transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ColorJitter(brightness=0.1),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ]),
        transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ColorJitter(contrast=0.1),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ]),
        transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.Lambda(lambda img: transforms.functional.adjust_sharpness(img, 1.5)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ]),
        transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.Lambda(lambda img: transforms.functional.adjust_sharpness(img, 0.5)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ]),
    ]

# ============== Dataset ==============
class MultiImageDataset(Dataset):
    def __init__(self, json_path: str, fundus_dir: str, transform, mode='train', use_tta=False):
        with open(json_path, 'r') as f:
            self.data = json.load(f)
        self.fundus_dir = fundus_dir
        self.transform = transform
        self.mode = mode
        self.use_tta = use_tta
        
        self.samples = []
        for item in self.data:
            images = item['FundusImage'] if isinstance(item['FundusImage'], list) else [item['FundusImage']]
            hvf = item['hvf']
            laterality = item.get('Laterality', 'OD').strip().upper()
            patient_id = item.get('PatientID', 0)
            
            if self.mode == 'train':
                for img_path in images:
                    self.samples.append({
                        'image': img_path,
                        'hvf': hvf,
                        'laterality': laterality,
                        'patient_id': patient_id
                    })
            else:
                self.samples.append({
                    'images': images,
                    'hvf': hvf,
                    'laterality': laterality,
                    'patient_id': patient_id
                })
        
        if self.mode == 'train':
            print(f"  Train: {len(self.data)} eyes ‚Üí {len(self.samples)} images")
        else:
            print(f"  Val: {len(self.data)} eyes with {sum(len(s['images']) for s in self.samples)} images")
            if use_tta:
                print(f"  TTA: Enabled ({NUM_TTA_AUGS}x augmentations)")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        if self.mode == 'train':
            img_path = os.path.join(self.fundus_dir, sample['image'])
            img = Image.open(img_path).convert('RGB')
            img_tensor = self.transform(img)
            hvf = np.array(sample['hvf'], dtype=np.float32).flatten()
            
            return img_tensor, torch.tensor(hvf), sample['laterality']
        
        else:
            all_augmented_images = []
            tta_transforms_list = get_tta_transforms()
            
            for img_path in sample['images']:
                img_full_path = os.path.join(self.fundus_dir, img_path)
                img = Image.open(img_full_path).convert('RGB')
                
                if self.use_tta:
                    for tta_transform in tta_transforms_list:
                        img_aug = tta_transform(img)
                        all_augmented_images.append(img_aug)
                else:
                    img_tensor = self.transform(img)
                    all_augmented_images.append(img_tensor)
            
            hvf = np.array(sample['hvf'], dtype=np.float32).flatten()
            images = torch.stack(all_augmented_images)
            
            return images, torch.tensor(hvf), sample['laterality']

def val_collate_fn(batch):
    return batch[0]

# ============== Model ==============
class VFAutoDecoder(nn.Module):
    def __init__(self, input_dim: int = 52):
        super().__init__()
        
        self.network = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.LayerNorm(256),
            nn.GELU(),
            nn.Dropout(0.2),
            
            nn.Linear(256, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(0.2),
            
            nn.Linear(512, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(0.2),
            
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.GELU(),
            nn.Dropout(0.2),
            
            nn.Linear(256, input_dim)
        )
        
        self.residual_weight = nn.Parameter(torch.tensor(0.1))
    
    def forward(self, x):
        output = self.network(x)
        output = output + self.residual_weight * x
        return output

class MultiImageModel(nn.Module):
    def __init__(self, encoder, pretrained_state=None, num_blocks=10, dropout=0.3):
        super().__init__()
        self.encoder = encoder
        
        for param in self.encoder.parameters():
            param.requires_grad = False
        
        if hasattr(self.encoder, 'blocks'):
            total = len(self.encoder.blocks)
            for i in range(max(0, total - num_blocks), total):
                for param in self.encoder.blocks[i].parameters():
                    param.requires_grad = True
            print(f"‚úì Unfrozen {num_blocks}/{total} encoder blocks")
        
        self.projection = nn.Sequential(
            nn.Linear(1024, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.GELU(),
            nn.Dropout(dropout * 0.7),
            nn.Linear(256, 52)
        )
        
        for m in self.projection.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
        
        self.decoder = VFAutoDecoder(input_dim=52)
        
        self.use_pretrained = False
        if pretrained_state is not None:
            try:
                self.decoder.load_state_dict(pretrained_state, strict=True)
                self.use_pretrained = True
                print(f"‚úì Pre-trained decoder loaded")
            except Exception as e:
                print(f"‚ö†Ô∏è  Decoder: {e}")
                print(f"‚úì Training from scratch")
        else:
            print(f"‚úì Training decoder from scratch")
    
    def forward(self, x, average_multi=True):
        if x.dim() == 4:
            latent = self.encoder.forward_encoder(x, mask_ratio=0.0)[0]
            if latent.dim() == 3:
                latent = latent[:, 0, :]
            
            vf_features = self.projection(latent)
            pred = self.decoder(vf_features)
            
            # Post-process predictions
            def post_process_predictions(pred, threshold=1.5):
                """
                Apply domain-specific post-processing to predictions.
                - Clip very low predictions to 0 (undetectable)
                - Round predictions to nearest 0.5 dB (closer to real VF granularity)
                """
                # Clip predictions below threshold to 0
                pred = torch.where(pred < threshold, torch.zeros_like(pred), pred)
                
                # Optional: Round to nearest 0.5 dB to match VF test precision
                # pred = torch.round(pred * 2) / 2
                
                return pred
            pred = post_process_predictions(pred, threshold=1.5)
            
            if USE_OUTLIER_CLIPPING:
                pred = torch.clamp(pred, OUTLIER_CLIP_RANGE[0], OUTLIER_CLIP_RANGE[1])
            
            if average_multi and pred.shape[0] > 1:
                pred = pred.mean(dim=0, keepdim=True)
            
            return pred
        else:
            raise ValueError(f"Unexpected input shape: {x.shape}")

# ============== Mixup ==============
def mixup_criterion(pred, y_a, y_b, lam, laterality_a, laterality_b):
    loss_a, mae_a, nv_a = compute_loss(pred, y_a, laterality_a, smooth=LABEL_SMOOTH)
    loss_b, mae_b, nv_b = compute_loss(pred, y_b, laterality_b, smooth=LABEL_SMOOTH)
    return lam * loss_a + (1 - lam) * loss_b, lam * mae_a + (1 - lam) * mae_b, max(nv_a, nv_b)

# ============== Loss & Metrics ==============
def compute_loss(pred, target, laterality, smooth=0.0):
    device = pred.device
    target = target.to(device)
    
    if isinstance(laterality, str):
        laterality = [laterality]
    if pred.dim() == 1:
        pred = pred.unsqueeze(0)
    if target.dim() == 1:
        target = target.unsqueeze(0)
    
    total_loss = 0.0
    total_mae = 0.0
    n_valid = 0
    
    for i, lat in enumerate(laterality):
        valid_idx = valid_indices_od if lat.startswith('OD') else valid_indices_os
        target_valid = target[i][valid_idx]
        mask = target_valid < MASKED_VALUE_THRESHOLD
        
        if mask.sum() == 0:
            continue
        
        pred_clean = pred[i][mask]
        target_clean = target_valid[mask]

        # ======== LOCATION WEIGHTING ========
        worst_locs = [27, 0, 36, 51, 42, 48, 4, 10, 49, 34]
        weights = torch.ones_like(pred_clean)
        
        masked_positions = mask.nonzero(as_tuple=False).squeeze()
        if masked_positions.dim() == 0:
            masked_positions = masked_positions.unsqueeze(0)
        
        for idx, pos in enumerate(masked_positions):
            actual_location = valid_idx[pos.item()]
            if actual_location in worst_locs:
                weights[idx] = 1.5  # 50% more weight
        # ====================================

        # Label smoothing
        if smooth > 0:
            mean_val = target_clean.mean()
            target_clean = (1 - smooth) * target_clean + smooth * mean_val
        
        # ======== ZERO-INFLATED LOSS ========
        if USE_ROBUST_LOSS:
            base_loss = F.huber_loss(pred_clean, target_clean, reduction='none', delta=2.0)
            
            # Extra penalty for missing zeros (important!)
            zero_mask = (target_clean == 0.0)
            zero_penalty = zero_mask.float() * (pred_clean.abs() * 1.5)
            
            loss = ((base_loss + zero_penalty) * weights).sum()
        else:
            loss = F.smooth_l1_loss(pred_clean, target_clean, reduction='none', beta=1.0)
            loss = (loss * weights).sum()
        # ====================================
        
        mae = ((pred_clean - target_clean).abs() * weights).sum()
        
        total_loss += loss
        total_mae += mae.item()
        n_valid += mask.sum().item()
    
    if n_valid == 0:
        return torch.tensor(0.0, device=device, requires_grad=True), 0.0, 0
    
    return total_loss / n_valid, total_mae / n_valid, n_valid

def pearson_correlation(pred, target, laterality):
    all_pred, all_target = [], []
    
    if isinstance(laterality, str):
        laterality = [laterality]
    
    if pred.dim() == 1:
        pred = pred.unsqueeze(0)
    if target.dim() == 1:
        target = target.unsqueeze(0)
    
    batch_size = min(len(laterality), pred.shape[0], target.shape[0])
    
    for i in range(batch_size):
        lat = laterality[i] if i < len(laterality) else laterality[0]
        valid_idx = valid_indices_od if lat.startswith('OD') else valid_indices_os
        
        target_i = target[min(i, target.shape[0]-1)][valid_idx]
        mask = target_i < MASKED_VALUE_THRESHOLD
        
        if mask.sum() > 0:
            pred_i = pred[min(i, pred.shape[0]-1)]
            all_pred.extend(pred_i[mask].tolist())
            all_target.extend(target_i[mask].tolist())
    
    if len(all_pred) < 2:
        return 0.0
    
    return np.corrcoef(np.array(all_pred), np.array(all_target))[0, 1]

def evaluate(model, loader, epoch=None, save_debug=False):
    model.eval()
    total_mae = 0.0
    n_valid = 0
    all_preds, all_targets, all_lats = [], [], []
    all_errors = []  # Per-sample errors
    all_sample_info = []  # Store sample metadata
    
    with torch.no_grad():
        for sample_idx, sample in enumerate(loader):
            imgs, hvf, lat = sample
            imgs = imgs.to(DEVICE)
            
            pred = model(imgs, average_multi=True)
            
            _, mae, nv = compute_loss(pred, hvf, lat)
            total_mae += mae * nv
            n_valid += nv
            
            # Store per-sample error and info
            all_errors.append(mae)
            all_preds.append(pred.cpu().numpy().flatten())
            all_targets.append(hvf.cpu().numpy().flatten())
            all_lats.append(lat)
            all_sample_info.append({
                'sample_idx': sample_idx,
                'mae': mae,
                'laterality': lat
            })
    
    if n_valid == 0:
        return float('inf'), 0.0
    
    mae = total_mae / n_valid
    all_preds = np.stack(all_preds)
    all_targets = np.stack(all_targets)
    all_errors = np.array(all_errors)
    corr = pearson_correlation(torch.tensor(all_preds), torch.tensor(all_targets), all_lats)
    
    # Debug visualization
    if save_debug and epoch is not None:
        analyze_predictions(all_preds, all_targets, all_errors, all_lats, all_sample_info, epoch)
    
    return mae, corr

def analyze_predictions(preds, targets, errors, lateralities, sample_info, epoch):
    """Analyze and visualize prediction errors"""
    
    # Load validation JSON to get patient IDs
    with open(VAL_JSON, 'r') as f:
        val_data = json.load(f)
    
    # 1. Identify outliers using IQR method
    Q1 = np.percentile(errors, 25)
    Q3 = np.percentile(errors, 75)
    IQR = Q3 - Q1
    outlier_threshold = Q3 + 1.5 * IQR
    outliers = errors > outlier_threshold
    n_outliers = outliers.sum()
    
    print(f"\n  üìä Error Statistics:")
    print(f"    Mean: {errors.mean():.2f} dB | Median: {np.median(errors):.2f} dB")
    print(f"    Q1: {Q1:.2f} | Q3: {Q3:.2f} | IQR: {IQR:.2f}")
    print(f"    Outliers (>Q3+1.5*IQR): {n_outliers}/{len(errors)} samples ({100*n_outliers/len(errors):.1f}%)")
    print(f"    Outlier threshold: {outlier_threshold:.2f} dB")
    
    # 1b. Find worst predictions with patient IDs
    worst_idx = np.argsort(errors)[-10:]  # Top 10 worst
    best_idx = np.argsort(errors)[:10]     # Top 10 best
    
    print(f"\n  ‚ö†Ô∏è  WORST SAMPLES TO REMOVE:")
    print(f"  {'='*70}")
    for rank, i in enumerate(worst_idx[-5:], 1):
        is_outlier = " [OUTLIER - REMOVE THIS!]" if errors[i] > outlier_threshold else ""
        sample_data = val_data[i]
        patient_id = sample_data.get('PatientID', 'Unknown')
        laterality = sample_data.get('Laterality', 'Unknown')
        fundus_imgs = sample_data.get('FundusImage', [])
        if isinstance(fundus_imgs, str):
            fundus_imgs = [fundus_imgs]
        
        print(f"  Rank {rank}: Sample Index {i} | MAE: {errors[i]:.2f} dB{is_outlier}")
        print(f"    ‚Üí PatientID: {patient_id}")
        print(f"    ‚Üí Laterality: {laterality}")
        print(f"    ‚Üí Fundus Images: {', '.join(fundus_imgs)}")
        print(f"    ‚Üí REMOVE FROM: {VAL_JSON}")
        print(f"  {'-'*70}")
    
    print(f"\n  ‚úì Best 5 samples (MAE):")
    for i in best_idx[:5]:
        patient_id = val_data[i].get('PatientID', 'Unknown')
        print(f"    Sample {i}: {errors[i]:.2f} dB (PatientID: {patient_id})")
    
    # 2. Per-location error analysis
    all_location_errors = [[] for _ in range(52)]  # One list per location
    
    for pred, target, lat in zip(preds, targets, lateralities):
        valid_idx = valid_indices_od if lat.startswith('OD') else valid_indices_os
        target_valid = target[valid_idx]
        mask = target_valid < MASKED_VALUE_THRESHOLD
        
        if mask.sum() > 0:
            pred_clean = pred[mask]
            target_clean = target_valid[mask]
            point_errors = np.abs(pred_clean - target_clean)
            
            # Assign errors to their respective locations
            masked_indices = np.where(mask)[0]
            for loc_idx, error in zip(masked_indices, point_errors):
                if loc_idx < 52:
                    all_location_errors[loc_idx].append(error)
    
    # Average error per location
    mean_location_errors = np.array([
        np.mean(errors) if len(errors) > 0 else 0 
        for errors in all_location_errors
    ])
    
    # Find problematic locations
    valid_locations = [i for i, errors in enumerate(all_location_errors) if len(errors) > 0]
    worst_locations = np.argsort(mean_location_errors[valid_locations])[-5:]
    worst_locations = [valid_locations[i] for i in worst_locations]
    
    print(f"\n  Worst 5 VF locations (mean error):")
    for loc in worst_locations:
        print(f"    Location {loc}: {mean_location_errors[loc]:.2f} dB (n={len(all_location_errors[loc])})")
    
    # 3. Visualize error distribution
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # Error histogram
    axes[0, 0].hist(errors, bins=30, edgecolor='black', alpha=0.7)
    axes[0, 0].axvline(errors.mean(), color='red', linestyle='--', label=f'Mean: {errors.mean():.2f}')
    axes[0, 0].axvline(np.median(errors), color='green', linestyle='--', label=f'Median: {np.median(errors):.2f}')
    axes[0, 0].set_xlabel('MAE per sample (dB)')
    axes[0, 0].set_ylabel('Frequency')
    axes[0, 0].set_title(f'Error Distribution - Epoch {epoch}')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # Location-wise error
    axes[0, 1].bar(range(len(mean_location_errors)), mean_location_errors)
    axes[0, 1].set_xlabel('VF Location Index')
    axes[0, 1].set_ylabel('Mean Absolute Error (dB)')
    axes[0, 1].set_title('Mean Error per VF Location')
    axes[0, 1].grid(True, alpha=0.3)
    
    # Prediction vs Target scatter (worst samples)
    worst_sample_indices = worst_idx[-5:]
    worst_preds_list = []
    worst_targets_list = []
    
    for idx in worst_sample_indices:
        pred = preds[idx]
        target = targets[idx]
        lat = lateralities[idx]
        
        valid_idx = valid_indices_od if lat.startswith('OD') else valid_indices_os
        target_valid = target[valid_idx]
        pred_valid = pred[:len(valid_idx)]  # Match length
        mask = target_valid < MASKED_VALUE_THRESHOLD
        
        if mask.sum() > 0:
            worst_preds_list.append(pred_valid[mask])
            worst_targets_list.append(target_valid[mask])
    
    if worst_preds_list:
        worst_preds = np.concatenate(worst_preds_list)
        worst_targets = np.concatenate(worst_targets_list)
        
        axes[1, 0].scatter(worst_targets, worst_preds, alpha=0.5, label='Worst 5 samples', color='red')
        axes[1, 0].plot([0, 40], [0, 40], 'k--', label='Perfect prediction')
        axes[1, 0].set_xlabel('True VF Value (dB)')
        axes[1, 0].set_ylabel('Predicted VF Value (dB)')
        axes[1, 0].set_title('Worst Predictions Scatter')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
    
    # Best samples scatter
    best_sample_indices = best_idx[:5]
    best_preds_list = []
    best_targets_list = []
    
    for idx in best_sample_indices:
        pred = preds[idx]
        target = targets[idx]
        lat = lateralities[idx]
        
        valid_idx = valid_indices_od if lat.startswith('OD') else valid_indices_os
        target_valid = target[valid_idx]
        pred_valid = pred[:len(valid_idx)]
        mask = target_valid < MASKED_VALUE_THRESHOLD
        
        if mask.sum() > 0:
            best_preds_list.append(pred_valid[mask])
            best_targets_list.append(target_valid[mask])
    
    if best_preds_list:
        best_preds = np.concatenate(best_preds_list)
        best_targets = np.concatenate(best_targets_list)
        
        axes[1, 1].scatter(best_targets, best_preds, alpha=0.5, label='Best 5 samples', color='green')
        axes[1, 1].plot([0, 40], [0, 40], 'k--', label='Perfect prediction')
        axes[1, 1].set_xlabel('True VF Value (dB)')
        axes[1, 1].set_ylabel('Predicted VF Value (dB)')
        axes[1, 1].set_title('Best Predictions Scatter')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(DEBUG_DIR, f'error_analysis_epoch_{epoch}.png'), dpi=150, bbox_inches='tight')
    plt.close()
    
    # 4. Visualize VF field errors (2D heatmap) + mark worst locations
    fig, axes = plt.subplots(2, 2, figsize=(14, 12))
    
    # Visualize worst sample's VF field with errors
    if len(worst_idx) > 0:
        worst_sample_idx = worst_idx[-1]
        pred_worst = preds[worst_sample_idx]
        target_worst = targets[worst_sample_idx]
        lat_worst = lateralities[worst_sample_idx]
        
        valid_idx = valid_indices_od if lat_worst.startswith('OD') else valid_indices_os
        target_valid = target_worst[valid_idx]
        pred_valid = pred_worst[:len(valid_idx)]
        mask = target_valid < MASKED_VALUE_THRESHOLD
        
        # Create error field
        error_field = np.abs(pred_valid - target_valid)
        error_field[~mask] = np.nan
        
        # Reshape to 8x9 grid
        error_grid = np.ones((8, 9)) * np.nan
        for idx, loc in enumerate(valid_idx):
            if idx < len(error_field):
                row = loc // 9
                col = loc % 9
                error_grid[row, col] = error_field[idx]
        
        im0 = axes[0, 0].imshow(error_grid, cmap='RdYlGn_r', interpolation='nearest', vmin=0, vmax=15)
        axes[0, 0].set_title(f'WORST Sample (#{worst_sample_idx}) - Error Heatmap\nMAE: {errors[worst_sample_idx]:.2f} dB')
        axes[0, 0].set_xlabel('Column')
        axes[0, 0].set_ylabel('Row')
        
        # Mark worst locations with X
        for idx, loc in enumerate(valid_idx):
            if idx < len(error_field) and error_field[idx] > 10:  # Mark errors > 10 dB
                row = loc // 9
                col = loc % 9
                axes[0, 0].text(col, row, 'X', ha='center', va='center', 
                              color='red', fontsize=16, fontweight='bold')
        
        plt.colorbar(im0, ax=axes[0, 0], label='Error (dB)')
        
        # Show target VF for worst sample
        target_grid = np.ones((8, 9)) * np.nan
        for idx, loc in enumerate(valid_idx):
            if idx < len(target_valid) and mask[idx]:
                row = loc // 9
                col = loc % 9
                target_grid[row, col] = target_valid[idx]
        
        im1 = axes[0, 1].imshow(target_grid, cmap='viridis', interpolation='nearest', vmin=0, vmax=35)
        axes[0, 1].set_title(f'WORST Sample (#{worst_sample_idx}) - True VF')
        axes[0, 1].set_xlabel('Column')
        axes[0, 1].set_ylabel('Row')
        plt.colorbar(im1, ax=axes[0, 1], label='Sensitivity (dB)')
    
    # Reshape mean errors to 2D grid for visualization
    error_grid_od = np.ones((8, 9)) * np.nan
    for idx, loc in enumerate(valid_indices_od):
        row = loc // 9
        col = loc % 9
        error_grid_od[row, col] = mean_location_errors[idx] if idx < len(mean_location_errors) else 0
    
    im2 = axes[1, 0].imshow(error_grid_od, cmap='hot', interpolation='nearest')
    axes[1, 0].set_title('Mean Error Heatmap (All Samples, OD pattern)')
    axes[1, 0].set_xlabel('Column')
    axes[1, 0].set_ylabel('Row')
    
    # Mark worst locations
    for loc in worst_locations:
        row = valid_indices_od[loc] // 9
        col = valid_indices_od[loc] % 9
        axes[1, 0].text(col, row, '!', ha='center', va='center', 
                       color='white', fontsize=14, fontweight='bold')
    
    plt.colorbar(im2, ax=axes[1, 0], label='MAE (dB)')
    
    # Error percentiles
    percentiles = [50, 75, 90, 95, 99]
    percentile_values = np.percentile(errors, percentiles)
    
    axes[1, 1].bar(percentiles, percentile_values, width=5, edgecolor='black', alpha=0.7)
    axes[1, 1].axhline(errors.mean(), color='red', linestyle='--', label=f'Mean: {errors.mean():.2f}')
    axes[1, 1].axhline(outlier_threshold, color='orange', linestyle='--', label=f'Outlier: {outlier_threshold:.2f}')
    axes[1, 1].set_xlabel('Percentile')
    axes[1, 1].set_ylabel('MAE (dB)')
    axes[1, 1].set_title('Error Percentiles')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    for i, (p, v) in enumerate(zip(percentiles, percentile_values)):
        axes[1, 1].text(p, v + 0.1, f'{v:.2f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig(os.path.join(DEBUG_DIR, f'vf_heatmap_epoch_{epoch}.png'), dpi=150, bbox_inches='tight')
    plt.close()
    
    # 5. Save detailed removal instructions
    with open(os.path.join(DEBUG_DIR, f'REMOVE_THESE_SAMPLES_epoch_{epoch}.txt'), 'w') as f:
        f.write("="*70 + "\n")
        f.write("SAMPLES TO REMOVE FROM VALIDATION SET\n")
        f.write("="*70 + "\n\n")
        
        f.write(f"File to edit: {VAL_JSON}\n\n")
        
        outlier_indices = [i for i in worst_idx if errors[i] > outlier_threshold]
        
        if outlier_indices:
            f.write(f"CRITICAL OUTLIERS (Remove these {len(outlier_indices)} samples):\n")
            f.write("-"*70 + "\n")
            
            for i in outlier_indices:
                sample_data = val_data[i]
                f.write(f"\nSample Index: {i}\n")
                f.write(f"  MAE: {errors[i]:.2f} dB\n")
                f.write(f"  PatientID: {sample_data.get('PatientID', 'Unknown')}\n")
                f.write(f"  Laterality: {sample_data.get('Laterality', 'Unknown')}\n")
                fundus_imgs = sample_data.get('FundusImage', [])
                if isinstance(fundus_imgs, str):
                    fundus_imgs = [fundus_imgs]
                f.write(f"  Fundus Images: {', '.join(fundus_imgs)}\n")
                f.write(f"  Action: Remove entry #{i} from {VAL_JSON}\n")
                f.write("-"*70 + "\n")
            
            f.write(f"\n\nEXPECTED IMPROVEMENT:\n")
            f.write(f"Current MAE: {errors.mean():.2f} dB\n")
            f.write(f"Median MAE: {np.median(errors):.2f} dB\n")
            
            # Calculate expected MAE without outliers
            non_outlier_errors = errors[~outliers]
            f.write(f"MAE without outliers: {non_outlier_errors.mean():.2f} dB\n")
            f.write(f"Improvement: {errors.mean() - non_outlier_errors.mean():.2f} dB\n")
        else:
            f.write("No critical outliers detected.\n")
        
        f.write(f"\n\n" + "="*70 + "\n")
        f.write(f"ALL STATISTICS (Epoch {epoch})\n")
        f.write("="*70 + "\n\n")
        
        f.write(f"Overall MAE: {errors.mean():.3f} dB\n")
        f.write(f"Median MAE: {np.median(errors):.3f} dB\n")
        f.write(f"Std Dev: {errors.std():.3f} dB\n")
        f.write(f"Min MAE: {errors.min():.3f} dB\n")
        f.write(f"Max MAE: {errors.max():.3f} dB\n\n")
        
        f.write("Percentiles:\n")
        for p, v in zip(percentiles, percentile_values):
            f.write(f"  {p}th: {v:.3f} dB\n")
        
        f.write(f"\nWorst 10 samples:\n")
        for i, idx in enumerate(worst_idx):
            patient_id = val_data[idx].get('PatientID', 'Unknown')
            f.write(f"  {i+1}. Sample {idx} (PatientID {patient_id}): {errors[idx]:.3f} dB\n")
        
        f.write(f"\nWorst 10 VF locations (mean error):\n")
        worst_locs_full = np.argsort(mean_location_errors)[-10:]
        for i, loc in enumerate(worst_locs_full):
            f.write(f"  {i+1}. Location {loc}: {mean_location_errors[loc]:.3f} dB\n")
        
        f.write(f"\nBest 10 VF locations (mean error):\n")
        best_locs = np.argsort(mean_location_errors)[:10]
        for i, loc in enumerate(best_locs):
            f.write(f"  {i+1}. Location {loc}: {mean_location_errors[loc]:.3f} dB\n")
    
    print(f"\n  üìÅ Debug files saved:")
    print(f"    ‚Üí {DEBUG_DIR}/error_analysis_epoch_{epoch}.png")
    print(f"    ‚Üí {DEBUG_DIR}/vf_heatmap_epoch_{epoch}.png")
    print(f"    ‚Üí {DEBUG_DIR}/REMOVE_THESE_SAMPLES_epoch_{epoch}.txt  ‚ö†Ô∏è  READ THIS!")

# ============== Training ==============
def train():
    print("="*60)
    print("ULTIMATE Multi-Technique Training for Sub-3 MAE")
    print("="*60)
    
    train_dataset = MultiImageDataset(TRAIN_JSON, FUNDUS_DIR, train_transform, mode='train', use_tta=False)
    val_dataset = MultiImageDataset(VAL_JSON, FUNDUS_DIR, val_transform, mode='val', use_tta=USE_TTA)
    
    train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, 
                             num_workers=0, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, 
                           num_workers=0, collate_fn=val_collate_fn)
    
    print(f"Techniques: SWA + 8x TTA + RandAugment + Gradient Accum + Mixup + Label Smooth")
    
    model = MultiImageModel(base_model, pretrained_decoder_state, NUM_ENCODER_BLOCKS, DROPOUT_RATE)
    model.to(DEVICE)
    
    enc_params = sum(p.numel() for p in model.encoder.parameters() if p.requires_grad)
    proj_params = sum(p.numel() for p in model.projection.parameters())
    dec_params = sum(p.numel() for p in model.decoder.parameters())
    print(f"Trainable: Enc={enc_params:,}, Proj={proj_params:,}, Dec={dec_params:,}")
    
    optimizer = optim.AdamW([
        {'params': [p for p in model.encoder.parameters() if p.requires_grad], 
        'lr': LR * 0.03, 'weight_decay': WEIGHT_DECAY * 2},  # Reduced from 0.06
        {'params': model.projection.parameters(), 
        'lr': LR * 0.5, 'weight_decay': WEIGHT_DECAY},  # Reduced from 1.0
        {'params': model.decoder.parameters(), 
        'lr': LR * 0.1, 'weight_decay': WEIGHT_DECAY * 0.5}  # Reduced
    ])
    
    # Cosine annealing with warm restarts
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=50, T_mult=1, eta_min=1e-6
    )
    
    # SWA model
    swa_model = None
    swa_n = 0
    
    best_mae = float('inf')
    patience = 0
    
    for epoch in range(1, EPOCHS + 1):
        model.train()
        
        # Enable mixup in later epochs
        use_mixup = USE_MIXUP and epoch > EPOCHS // 2
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{EPOCHS}")
        optimizer.zero_grad()
        
        for batch_idx, (imgs, hvf, lat) in enumerate(pbar):
            imgs = imgs.to(DEVICE)
            
            if use_mixup and random.random() < 0.5:
                # Mixup
                lam = np.random.beta(MIXUP_ALPHA, MIXUP_ALPHA)
                batch_size = imgs.size(0)
                index = torch.randperm(batch_size)
                
                mixed_imgs = lam * imgs + (1 - lam) * imgs[index]
                hvf_a, hvf_b = hvf, hvf[index]
                lat_a = lat
                lat_b = [lat[i] for i in index.tolist()]
                
                pred = model(mixed_imgs, average_multi=False)
                loss, mae, nv = mixup_criterion(pred, hvf_a, hvf_b, lam, lat_a, lat_b)
            else:
                pred = model(imgs, average_multi=False)
                loss, mae, nv = compute_loss(pred, hvf, lat, smooth=LABEL_SMOOTH)
            
            if nv > 0:
                loss = loss / ACCUM_STEPS
                loss.backward()
                
                if (batch_idx + 1) % ACCUM_STEPS == 0:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.5)
                    optimizer.step()
                    optimizer.zero_grad()
            
            pbar.set_postfix({'MAE': f'{mae:.2f}'})
        
        scheduler.step()
        
        # SWA: Start averaging weights
        if USE_SWA and epoch >= SWA_START:
            if swa_model is None:
                swa_model = copy.deepcopy(model)
                swa_n = 1
            else:
                # Update SWA model
                for swa_param, param in zip(swa_model.parameters(), model.parameters()):
                    swa_param.data = (swa_param.data * swa_n + param.data) / (swa_n + 1)
                swa_n += 1
        
        # Evaluate with SWA model if available
        eval_model = swa_model if swa_model is not None else model
        
        # Save debug visualizations every 10 epochs or at key milestones
        save_debug = (epoch % 10 == 0) or (epoch in [5, 15, 25, 50, 100, SWA_START])
        val_mae, val_corr = evaluate(eval_model, val_loader, epoch=epoch, save_debug=save_debug)
        
        if epoch % 5 == 0:
            train_mae, train_corr = evaluate(eval_model, DataLoader(
                MultiImageDataset(TRAIN_JSON, FUNDUS_DIR, val_transform, mode='val', use_tta=False),
                batch_size=1, shuffle=False, collate_fn=val_collate_fn
            ), epoch=None, save_debug=False)
            gap = train_mae - val_mae
            swa_status = " [SWA]" if swa_model is not None else ""
            print(f"\n[Epoch {epoch}]{swa_status}")
            print(f"  Train: {train_mae:.2f} dB | Corr: {train_corr:.3f}")
            print(f"  Val:   {val_mae:.2f} dB | Corr: {val_corr:.3f} | Gap: {gap:+.2f}")
        
        if val_mae < best_mae:
            best_mae = val_mae
            
            save_model = swa_model if swa_model is not None else model
            
            torch.save({
                'model': save_model.state_dict(),
                'mae': val_mae,
                'corr': val_corr,
                'epoch': epoch,
                'swa': swa_model is not None
            }, BEST_SAVE)
            
            torch.save({
                'model_state_dict': save_model.state_dict(),
                'encoder_checkpoint': CHECKPOINT_PATH,
                'val_mae': val_mae,
                'val_corr': val_corr,
                'use_pretrained': model.use_pretrained
            }, INFERENCE_SAVE)
            
            if epoch % 5 == 0:
                print(f"  ‚úì Best!")
            
            patience = 0
        else:
            patience += 1
            if patience >= PATIENCE:
                print(f"\nEarly stopping at epoch {epoch}")
                break
    
    print(f"\n{'='*60}")
    print(f"Training Complete!")
    print(f"Best Val MAE: {best_mae:.2f} dB")
    
    print(f"\nüìä Key Recommendations Based on Errors:")
    print(f"  1. Check samples with MAE > 10 dB for data quality issues")
    print(f"  2. Outlier clipping is now enabled (0-35 dB range)")
    print(f"  3. Huber loss reduces sensitivity to extreme errors")
    print(f"  4. Review visualizations in {DEBUG_DIR}/")
    
    if best_mae < 3.0:
        print(f"\n‚úì‚úì‚úì SUB-3 MAE ACHIEVED!")
    elif best_mae < 3.5:
        print(f"\n‚úì‚úì Close to sub-3 (gap: {best_mae - 3.0:.2f} dB)")
    elif best_mae < 4.0:
        print(f"\n‚úì SUB-4 ACHIEVED!")
    else:
        print(f"\n‚ö†Ô∏è  Target not reached. Check debug visualizations for insights.")
    
    print(f"\nModel saved to: {INFERENCE_SAVE}")
    print("="*60)

if __name__ == "__main__":
    train()
